{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f243e1-dd49-4af4-af18-0b46da947573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "# pip install python-dotenv\n",
    "\n",
    "# Add the following to .env file\n",
    "# OPENAI_ORG_KEY=org-..\n",
    "# OPENAI_API_KEY=sk-..\n",
    "# TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17127559-b32a-46cb-857b-ae07d0d24253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using langchain API\n",
    "# pip install langchain\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "model = OpenAI() # requires 'OPENAI_API_KEY' env var to be set\n",
    "response = model(\"What is the capital of france?\")\n",
    "response.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6130e5f-e936-470c-9370-d814d462e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using REST API\n",
    "# pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # requires 'OPENAI_API_KEY' env var to be set\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2694ab9-0e70-4ab2-8ec8-3a653665d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG - Retrieval Augumented Generation\n",
    "# pip install requests beautifulsoup4 sentence-transformers faiss-cpu openai\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import faiss\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_answer(query, url):\n",
    "    # Download contents of a web page\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Generate Text Embeddings from the web page content\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') # Downloads the embedding model from the internet\n",
    "    sentences = text.split('.')\n",
    "    sentences = [re.sub(r'\\s+', ' ', s) for s in sentences]\n",
    "    embeddings = model.encode(sentences) # One embedding per sentence\n",
    "    \n",
    "    # Storing and Indexing Content to a Vector Database (using FAISS for now, can use any vector db)\n",
    "    \n",
    "    # Initialize a FAISS index\n",
    "    dimension = embeddings.shape[1]  # Dimension of the embeddings\n",
    "    base_index = faiss.IndexFlatL2(dimension)\n",
    "    index = faiss.IndexIDMap(base_index) # So that we can save an id in the index\n",
    "    \n",
    "    # Todo : save the senteneces somewhere permanent and then use the permanent ids instead.\n",
    "    \n",
    "    # Add vectors to the index\n",
    "    ids = np.arange(len(embeddings))\n",
    "    index.add_with_ids(np.array(embeddings).astype('float32'), ids)\n",
    "    \n",
    "    # Find ids of embeddings from the index which are close to the query\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), k=3)\n",
    "    \n",
    "    debug=False\n",
    "    if debug:\n",
    "        # Print top k similar sentences\n",
    "        for i in indices[0]: # Since we only have one item in [query] use 'indices[0]'\n",
    "            print(sentences[i])\n",
    "            print('-'*50)\n",
    "    \n",
    "    context = \".\".join([sentences[i] for i in indices[0]])\n",
    "    \n",
    "    # Do an LLM Query with the context\n",
    "    client = OpenAI()  # requires 'OPENAI_API_KEY' env var to be set\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {query} \\n Use the following context if it is helpful: {context}\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Use the contents of the URL to get answer to the question\n",
    "query = \"What is work study position\"\n",
    "url = \"https://studentlife.utoronto.ca/news/work-study-is-back-for-september-2022/\"\n",
    "query = \"Why are stripped stars difficult to find?\"\n",
    "url = \"https://www.utoronto.ca/news/u-t-astronomers-discover-first-population-binary-stripped-stars\"\n",
    "\n",
    "get_answer(query, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2b177-49c5-4c5a-944b-be44d1aa8173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
